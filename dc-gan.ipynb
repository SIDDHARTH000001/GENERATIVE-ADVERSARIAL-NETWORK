{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7457578,"sourceType":"datasetVersion","datasetId":1698586}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport torch\nimport os\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nimport zipfile\nfrom io import BytesIO\nfrom PIL import Image\nimport numpy as np\nfrom torchvision.utils import save_image","metadata":{"execution":{"iopub.status.busy":"2024-06-20T05:35:48.567486Z","iopub.execute_input":"2024-06-20T05:35:48.568218Z","iopub.status.idle":"2024-06-20T05:35:51.300367Z","shell.execute_reply.started":"2024-06-20T05:35:48.568176Z","shell.execute_reply":"2024-06-20T05:35:51.299525Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"batch_size = 16\nlatent_space = 100\n_channels = 3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nnum_epochs = 300\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-20T05:35:51.303688Z","iopub.execute_input":"2024-06-20T05:35:51.304151Z","iopub.status.idle":"2024-06-20T05:35:51.330927Z","shell.execute_reply.started":"2024-06-20T05:35:51.304116Z","shell.execute_reply":"2024-06-20T05:35:51.329810Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import zipfile\nfrom PIL import Image\nfrom io import BytesIO\nfrom torchvision import transforms\nimport torch\n\ndef image_batch_generator(zip_file, batch_size, transform=None):\n    with zipfile.ZipFile(zip_file, 'r') as z:\n        image_files = [f for f in z.namelist() if f.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))]\n        total_images = len(image_files)\n\n        for i in range(0, total_images, batch_size):\n            batch_files = image_files[i:i + batch_size]\n            images = []\n            for filename in batch_files:\n                with z.open(filename) as file:\n                    img = Image.open(BytesIO(file.read())).convert('RGB')\n                    if transform:\n                        img = transform(img)\n                    images.append(img)\n            yield torch.stack(images)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-20T05:35:51.332039Z","iopub.execute_input":"2024-06-20T05:35:51.332345Z","iopub.status.idle":"2024-06-20T05:35:51.349830Z","shell.execute_reply.started":"2024-06-20T05:35:51.332319Z","shell.execute_reply":"2024-06-20T05:35:51.348875Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport torch\n\ndef image_batch_generator(root_folder, batch_size, transform=None):\n    all_image_files = []\n\n    # Walk through all subdirectories and collect image files\n    for root, _, files in os.walk(root_folder):\n        for file in files:\n            if file.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n                all_image_files.append(os.path.join(root, file))\n\n    total_images = len(all_image_files)\n    \n    for i in range(0, total_images, batch_size):\n        batch_files = all_image_files[i:i + batch_size]\n        images = []\n        for filename in batch_files:\n            img = Image.open(filename).convert('RGB')\n            if transform:\n                img = transform(img)\n            images.append(torch.tensor(img))\n        yield torch.stack(images)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-20T05:35:51.350901Z","iopub.execute_input":"2024-06-20T05:35:51.351189Z","iopub.status.idle":"2024-06-20T05:35:51.364476Z","shell.execute_reply.started":"2024-06-20T05:35:51.351145Z","shell.execute_reply":"2024-06-20T05:35:51.363684Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"zip_file_path = '/kaggle/input/art-portraits'\n\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n])\n\n_dataloader = image_batch_generator(zip_file_path, batch_size, transform=transform)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T05:35:51.366476Z","iopub.execute_input":"2024-06-20T05:35:51.366744Z","iopub.status.idle":"2024-06-20T05:35:51.374405Z","shell.execute_reply.started":"2024-06-20T05:35:51.366721Z","shell.execute_reply":"2024-06-20T05:35:51.373557Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## DESCRIMINATOR","metadata":{}},{"cell_type":"code","source":"class Descriminator(nn.Module):\n    def __init__(self, input_channels):\n        super(Descriminator, self).__init__()\n\n        self.channels = input_channels\n\n        self.conv1 = nn.Conv2d(self.channels, 32, kernel_size=3, stride=2, padding=1)\n        self.batchnorm1 = nn.BatchNorm2d(32)\n        self.activation1 = nn.LeakyReLU()\n        self.dropout1 = nn.Dropout(0.5)\n\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n        self.batchnorm2 = nn.BatchNorm2d(64)\n        self.activation2 = nn.LeakyReLU()\n        self.dropout2 = nn.Dropout(0.3)\n\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n        self.batchnorm3 = nn.BatchNorm2d(128)\n        self.activation3 = nn.LeakyReLU()\n        self.dropout3 = nn.Dropout(0.3)\n\n        self.flatten = nn.Flatten(1,-1)\n\n        self.linear1 = nn.Linear(128 * 32 * 32, 2048)\n        self.batchnorm4 = nn.BatchNorm1d(2048)\n        self.activation4 = nn.LeakyReLU()\n        \n        self.linear2 = nn.Linear(2048,1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self,x):\n        x = self.conv1(x)\n        x = self.batchnorm1(x)\n        x = self.activation1(x)\n        x = self.dropout1(x)\n        \n        x = self.conv2(x)\n        x = self.batchnorm2(x)\n        x = self.activation2(x)\n        x = self.dropout2(x)\n\n        x = self.conv3(x)\n        x = self.batchnorm3(x)\n        x = self.activation3(x)\n        x = self.dropout3(x)\n        \n        x = self.flatten(x)\n\n        x = self.linear1(x)\n        x = self.batchnorm4(x)\n        x = self.activation4(x)\n        \n        x = self.linear2(x)\n        x = self.sigmoid(x)\n\n        return x\n\n    ","metadata":{"execution":{"iopub.status.busy":"2024-06-20T05:35:51.375408Z","iopub.execute_input":"2024-06-20T05:35:51.375726Z","iopub.status.idle":"2024-06-20T05:35:51.388563Z","shell.execute_reply.started":"2024-06-20T05:35:51.375695Z","shell.execute_reply":"2024-06-20T05:35:51.387749Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# decrimnator = Descriminator(input_channels=3)\n# x = decrimnator(_dataloader.__next__())","metadata":{"execution":{"iopub.status.busy":"2024-06-20T05:35:51.389650Z","iopub.execute_input":"2024-06-20T05:35:51.390042Z","iopub.status.idle":"2024-06-20T05:35:51.404034Z","shell.execute_reply.started":"2024-06-20T05:35:51.390004Z","shell.execute_reply":"2024-06-20T05:35:51.403203Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# x.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-20T05:35:51.405368Z","iopub.execute_input":"2024-06-20T05:35:51.405697Z","iopub.status.idle":"2024-06-20T05:35:51.414353Z","shell.execute_reply.started":"2024-06-20T05:35:51.405666Z","shell.execute_reply":"2024-06-20T05:35:51.413490Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# GENERATOR","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\n# output_shape = ( input - 1 ) * stride + output_padding - 2 * padding + kernel_size\n\nclass Generator(nn.Module):\n    def __init__(self, input_size, output_channels):\n        super(Generator, self).__init__()\n        self.input_size = input_size\n        self.channels = output_channels\n\n        # self.conv1 = nn.ConvTranspose2d(input_size, 1024, 5, 2, 0, bias=False)\n        # self.batchnorm1 = nn.BatchNorm2d(1024)\n        # self.activation1 = nn.ReLU()\n\n        self.conv2 = nn.ConvTranspose2d(input_size, 512, 4, 2, 0, bias=False)\n        self.batchnorm2 = nn.BatchNorm2d(512)\n        self.activation2 = nn.ReLU(True)\n\n        self.conv3 = nn.ConvTranspose2d(512, 256, 3, 2, 1,output_padding = 1, bias=False)\n        self.batchnorm3 = nn.BatchNorm2d(256)\n        self.activation3 = nn.ReLU(True)\n\n        self.conv4 = nn.ConvTranspose2d(256, 256, 3, 2, 1,output_padding = 1, bias=False)\n        self.batchnorm4 = nn.BatchNorm2d(256)\n        self.activation4 = nn.ReLU(True)\n\n        self.conv5 = nn.ConvTranspose2d(256, 128, 3, 2, 1, output_padding = 1,bias=False)\n        self.batchnorm5 = nn.BatchNorm2d(128)\n        self.activation5 = nn.ReLU(True)\n\n        self.conv6 = nn.ConvTranspose2d(128, 64, 3, 2, 1, output_padding = 1,bias=False)\n        self.batchnorm6 = nn.BatchNorm2d(64)\n        self.activation6 = nn.ReLU(True)\n\n        self.conv7 = nn.ConvTranspose2d(64, 32, 3, 2, 1,output_padding = 1, bias=False)\n        self.batchnorm7 = nn.BatchNorm2d(32)\n        self.activation7 = nn.ReLU(True)\n\n        self.conv8 = nn.ConvTranspose2d(32, self.channels, 3, 2, 1, output_padding = 1,bias=False)\n        self.batchnorm8 = nn.BatchNorm2d(self.channels)\n        self.activation8 = nn.Tanh()\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        x = x.view(batch_size, self.input_size, 1, 1)\n        # x = self.conv1(x)\n        # x = self.batchnorm1(x)\n        # x = self.activation1(x)\n\n        x = self.conv2(x)\n        x = self.batchnorm2(x)\n        x = self.activation2(x)\n\n        x = self.conv3(x)\n        x = self.batchnorm3(x)\n        x = self.activation3(x)\n\n        x = self.conv4(x)\n        x = self.batchnorm4(x)\n        x = self.activation4(x)\n\n        x = self.conv5(x)\n        x = self.batchnorm5(x)\n        x = self.activation5(x)\n\n        x = self.conv6(x)\n        x = self.batchnorm6(x)\n        x = self.activation6(x)\n\n        x = self.conv7(x)\n        x = self.batchnorm7(x)\n        x = self.activation7(x)\n\n        x = self.conv8(x)\n        x = self.batchnorm8(x)\n        x = self.activation8(x)\n       \n        return x\n    ","metadata":{"execution":{"iopub.status.busy":"2024-06-20T05:35:55.280620Z","iopub.execute_input":"2024-06-20T05:35:55.281717Z","iopub.status.idle":"2024-06-20T05:35:55.298095Z","shell.execute_reply.started":"2024-06-20T05:35:55.281670Z","shell.execute_reply":"2024-06-20T05:35:55.297152Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# generator_input = torch.randn(8, 100)  # Batch size of 64, input size of 100\n# generator = Generator(input_size=100,output_channels=3)\n# output = generator(generator_input)\n# print(output.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T05:35:55.836355Z","iopub.execute_input":"2024-06-20T05:35:55.836664Z","iopub.status.idle":"2024-06-20T05:35:55.840534Z","shell.execute_reply.started":"2024-06-20T05:35:55.836640Z","shell.execute_reply":"2024-06-20T05:35:55.839602Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# GENERATOR USING  UPSAMPLING","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass UpGenerator(nn.Module):\n    def __init__(self, input_size=100, output_channels=3):\n        super(UpGenerator, self).__init__()\n        self.input_size = input_size\n        self.channels = output_channels\n\n        self.fc = nn.Linear(input_size, 512 * 8 * 8)\n\n        self.conv1 = nn.Conv2d(512, 256, 3, 1, 1, bias=False)\n        self.batchnorm1 = nn.BatchNorm2d(256)\n        self.activation1 = nn.ReLU()\n\n\n        self.upsampling1 = nn.UpsamplingBilinear2d(scale_factor=2)\n        self.conv2 = nn.Conv2d(256, 128, 3, 1, 1, bias=False)\n        self.batchnorm2 = nn.BatchNorm2d(128)\n        self.activation2 = nn.ReLU()\n\n        self.upsampling2 = nn.UpsamplingBilinear2d(scale_factor=2)\n        self.conv3 = nn.Conv2d(128, 64, 3, 1, 1, bias=False)\n        self.batchnorm3 = nn.BatchNorm2d(64)\n        self.activation3 = nn.ReLU()\n\n        self.upsampling3 = nn.UpsamplingBilinear2d(scale_factor=2)\n        self.conv4 = nn.Conv2d(64, 32, 3, 1, 1, bias=False)\n        self.batchnorm4 = nn.BatchNorm2d(32)\n        self.activation4 = nn.ReLU()\n\n        self.upsampling4 = nn.UpsamplingBilinear2d(scale_factor=2)\n        self.conv5 = nn.Conv2d(32, 16, 3, 1, 1, bias=False)\n        self.batchnorm5 = nn.BatchNorm2d(16)\n        self.activation5 = nn.ReLU()\n\n        self.upsampling5 = nn.UpsamplingBilinear2d(scale_factor=2)\n        self.conv6 = nn.Conv2d(16, self.channels, 3, 1, 1, bias=False)\n        self.activation6 = nn.Tanh()\n\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        x = self.fc(x).view(batch_size, 512, 8, 8)\n\n        x = self.conv1(x)\n        x = self.batchnorm1(x)\n        x = self.activation1(x)\n\n        x = self.upsampling1(x)\n        x = self.conv2(x)\n        x = self.batchnorm2(x)\n        x = self.activation2(x)\n\n        x = self.upsampling2(x)\n        x = self.conv3(x)\n        x = self.batchnorm3(x)\n        x = self.activation3(x)\n\n        x = self.upsampling3(x)\n        x = self.conv4(x)\n        x = self.batchnorm4(x)\n        x = self.activation4(x)\n\n        x = self.upsampling4(x)\n        x = self.conv5(x)\n        x = self.batchnorm5(x)\n        x = self.activation5(x)\n\n        x = self.upsampling5(x)\n        x = self.conv6(x)\n        x = self.activation6(x)\n\n        return x\n\n# generator_input = torch.randn(8, 100)  # Batch size of 64, input size of 100\n# generator = UpGenerator(input_size=100,output_channels=3)\n# output = generator(generator_input)\n# print(output.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T05:35:57.153966Z","iopub.execute_input":"2024-06-20T05:35:57.154441Z","iopub.status.idle":"2024-06-20T05:35:57.170584Z","shell.execute_reply.started":"2024-06-20T05:35:57.154405Z","shell.execute_reply":"2024-06-20T05:35:57.169598Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"\nclass DCGAN(nn.Module):\n    def __init__(self, discriminator,generator, latent_dim, device, lr_gen=0.0002, lr_disc=0.0002, betas=(0.5, 0.999)):\n        super(DCGAN, self).__init__()\n        self.generator = generator.to(device)\n        self.discriminator = discriminator.to(device)\n        self.latent_dim = latent_dim\n        self.device = device\n        \n        self.optim_gen = optim.Adam(self.generator.parameters(), lr=lr_gen, betas=betas)\n        self.optim_disc = optim.Adam(self.discriminator.parameters(), lr=lr_disc, betas=betas)\n        \n        self.criterion = nn.BCELoss()\n        self.loss_fn = nn.BCELoss()\n        \n    def train_step(self, real_images):\n        batch_size = real_images.shape[0]\n        real_images = real_images.to(self.device)\n\n        random_latent_vectors = torch.randn(batch_size, self.latent_dim, device=self.device)\n\n        generated_images = self.generator(random_latent_vectors)\n\n        real_predictions = self.discriminator(real_images)\n        fake_predictions = self.discriminator(generated_images.detach())  # Detach to avoid backprop through generator\n\n        # Labels and label noise\n        real_labels = torch.ones_like(real_predictions, device=self.device)\n        real_noisy_labels = real_labels + 0.1 * torch.rand_like(real_labels, device=self.device)\n        fake_labels = torch.zeros_like(fake_predictions, device=self.device)\n        fake_noisy_labels = fake_labels - 0.1 * torch.rand_like(fake_labels, device=self.device)\n\n\n        real_noisy_labels = torch.clamp(real_noisy_labels,0,1)\n        fake_noisy_labels = torch.clamp(fake_noisy_labels,0,1)\n        \n        d_real_loss = self.loss_fn(real_predictions, real_noisy_labels)\n        d_fake_loss = self.loss_fn(fake_predictions, fake_noisy_labels)\n        d_loss = (d_real_loss + d_fake_loss) / 2.0\n\n\n        self.optim_disc.zero_grad()\n        d_loss.backward()\n        self.optim_disc.step()\n\n        fake_predictions = self.discriminator(generated_images)\n        g_loss = self.criterion(fake_predictions, real_labels) #+ nn.SmoothL1Loss()(generated_images, real_images)\n\n        # Backpropagation for generator\n        self.optim_gen.zero_grad()\n        g_loss.backward()\n        self.optim_gen.step()\n\n        return {\"d_loss\": d_loss.item(), \"g_loss\": g_loss.item()}\n    \n\n    def generate_images(self, num_images=1, save_path=None,epoch=None):\n        self.generator.eval()\n        if epoch is None:\n            epoch = \"_\"\t\n            \n        if os.path.exists(save_path) == False:\n            os.makedirs(save_path)\n\n        with torch.no_grad():\n            noise = torch.randn(num_images, self.latent_dim).to(self.device)\n            generated_images = self.generator(noise)\n            \n            if save_path is not None:\n                for i, image in enumerate(generated_images):\n                    if os.path.exists(save_path) == False:\n                        os.makedirs(save_path)\n                    save_image(image, f\"{save_path}/EPOCH_{epoch}_image_{i}.png\")\n            \n            return generated_images","metadata":{"execution":{"iopub.status.busy":"2024-06-20T05:35:57.878557Z","iopub.execute_input":"2024-06-20T05:35:57.879198Z","iopub.status.idle":"2024-06-20T05:35:57.895458Z","shell.execute_reply.started":"2024-06-20T05:35:57.879151Z","shell.execute_reply":"2024-06-20T05:35:57.894403Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# def weights_init_normal(m):\n#     if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n#         nn.init.normal_(m.weight, mean=0.0, std=0.02)\n#         if m.bias is not None:\n#             nn.init.zeros_(m.bias)\n# latent_space = 250\n\n# descriminator = Descriminator(input_channels=_channels).apply(weights_init_normal)\n# generator = Generator(input_size=latent_space,output_channels=_channels).apply(weights_init_normal)\n\n# DCGAN = DCGAN(descriminator, generator, latent_space, device, lr_gen= 0.00002, lr_disc=0.00002, betas=(0.5, 0.999))\n\ndef weights_init_glorot(m):\n    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n        nn.init.xavier_uniform_(m.weight)\n        if m.bias is not None:\n            nn.init.zeros_(m.bias)\n\n\ndescriminator = Descriminator(input_channels=_channels).apply(weights_init_glorot)\ngenerator = Generator(input_size=latent_space,output_channels=_channels).apply(weights_init_glorot)\n\nDCGAN = DCGAN(descriminator, generator, latent_space, device, lr_gen=2e-3, lr_disc=2e-3, betas=(0.5, 0.999))","metadata":{"execution":{"iopub.status.busy":"2024-06-20T05:36:00.229830Z","iopub.execute_input":"2024-06-20T05:36:00.230206Z","iopub.status.idle":"2024-06-20T05:36:05.381467Z","shell.execute_reply.started":"2024-06-20T05:36:00.230174Z","shell.execute_reply":"2024-06-20T05:36:05.380449Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"_dataloader = image_batch_generator(zip_file_path, 16, transform=transform)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T05:36:09.706956Z","iopub.execute_input":"2024-06-20T05:36:09.707304Z","iopub.status.idle":"2024-06-20T05:36:09.713819Z","shell.execute_reply.started":"2024-06-20T05:36:09.707277Z","shell.execute_reply":"2024-06-20T05:36:09.713001Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"DCGAN.train_step(_dataloader.__next__())\n","metadata":{"execution":{"iopub.status.busy":"2024-06-20T05:36:12.311491Z","iopub.execute_input":"2024-06-20T05:36:12.312337Z","iopub.status.idle":"2024-06-20T05:36:13.286993Z","shell.execute_reply.started":"2024-06-20T05:36:12.312302Z","shell.execute_reply":"2024-06-20T05:36:13.285955Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_472/3432998535.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  images.append(torch.tensor(img))\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"{'d_loss': 0.8332287073135376, 'g_loss': 4.658342361450195}"},"metadata":{}}]},{"cell_type":"code","source":"# import shutil\n# import os\n\n# # Specify the path to the folder you want to delete\n# folder_path = '/kaggle/working/generated_images'\n\n# # Check if the folder exists\n# if os.path.exists(folder_path):\n#     # Remove the folder and all its contents\n#     shutil.rmtree(folder_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(num_epochs):\n    x=0\n    _dataloader = image_batch_generator(zip_file_path, 32, transform=transform)\n    for idx,j in enumerate(_dataloader):\n        x = DCGAN.train_step(j)\n        print(f\"epoch :{i} batch: {idx}\",end =\"\\r\")\n    print(f\"epoch {i}: {x}\")\n    if i%15 == 0:\n        torch.save(DCGAN.state_dict(), 'model_state_dict.pth')\n    DCGAN.generate_images(num_images=1, save_path=\"generated_images\",epoch=i)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T05:36:28.566412Z","iopub.execute_input":"2024-06-20T05:36:28.566769Z","iopub.status.idle":"2024-06-20T05:56:16.185251Z","shell.execute_reply.started":"2024-06-20T05:36:28.566738Z","shell.execute_reply":"2024-06-20T05:56:16.183905Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_472/3432998535.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  images.append(torch.tensor(img))\n","output_type":"stream"},{"name":"stdout","text":"epoch 0: {'d_loss': 1.3815033435821533, 'g_loss': 6.308757305145264}\nepoch 1: {'d_loss': 1.3254845142364502, 'g_loss': 3.7382590770721436}\nepoch 2: {'d_loss': 1.0476481914520264, 'g_loss': 4.312037944793701}\nepoch 3: {'d_loss': 0.5357766151428223, 'g_loss': 4.355941295623779}\nepoch :4 batch: 77\r","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m _dataloader \u001b[38;5;241m=\u001b[39m image_batch_generator(zip_file_path, \u001b[38;5;241m32\u001b[39m, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx,j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(_dataloader):\n\u001b[1;32m      5\u001b[0m     x \u001b[38;5;241m=\u001b[39m DCGAN\u001b[38;5;241m.\u001b[39mtrain_step(j)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch :\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m batch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,end \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[4], line 22\u001b[0m, in \u001b[0;36mimage_batch_generator\u001b[0;34m(root_folder, batch_size, transform)\u001b[0m\n\u001b[1;32m     20\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(filename)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m transform:\n\u001b[0;32m---> 22\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     images\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mtensor(img))\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(images)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    354\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:490\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    488\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    489\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m--> 490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:2193\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   2186\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2187\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2188\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2189\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2190\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2191\u001b[0m         )\n\u001b[0;32m-> 2193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# Save only the model's state dict\n# torch.save(DCGAN.state_dict(), 'model_state_dict.pth')\n# os.remove(\"/kaggle/working/model_state_dict.pth\")\n# torch.save(DCGAN.state_dict(), 'model_state_dict.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DCGAN.generate_images(num_images=1, save_path=\"generated_images\",epoch=4001)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(i.numel() for i in DCGAN.parameters())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}