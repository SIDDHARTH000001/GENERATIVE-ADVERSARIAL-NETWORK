{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-06-20T05:35:48.568218Z","iopub.status.busy":"2024-06-20T05:35:48.567486Z","iopub.status.idle":"2024-06-20T05:35:51.300367Z","shell.execute_reply":"2024-06-20T05:35:51.299525Z","shell.execute_reply.started":"2024-06-20T05:35:48.568176Z"},"trusted":true},"outputs":[],"source":["from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","import torch\n","import os\n","import torch.nn as nn\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","import zipfile\n","from io import BytesIO\n","from PIL import Image\n","import numpy as np\n","from torchvision.utils import save_image"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-20T05:35:51.304151Z","iopub.status.busy":"2024-06-20T05:35:51.303688Z","iopub.status.idle":"2024-06-20T05:35:51.330927Z","shell.execute_reply":"2024-06-20T05:35:51.329810Z","shell.execute_reply.started":"2024-06-20T05:35:51.304116Z"},"trusted":true},"outputs":[],"source":["batch_size = 16\n","latent_space = 512\n","_channels = 3\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","num_epochs = 300\n","\n"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-06-20T05:35:51.332345Z","iopub.status.busy":"2024-06-20T05:35:51.332039Z","iopub.status.idle":"2024-06-20T05:35:51.349830Z","shell.execute_reply":"2024-06-20T05:35:51.348875Z","shell.execute_reply.started":"2024-06-20T05:35:51.332319Z"},"trusted":true},"outputs":[],"source":["import zipfile\n","from PIL import Image\n","from io import BytesIO\n","from torchvision import transforms\n","import torch\n","\n","def image_batch_generator(zip_file, batch_size, transform=None):\n","    with zipfile.ZipFile(zip_file, 'r') as z:\n","        image_files = [f for f in z.namelist() if f.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))]\n","        total_images = len(image_files)\n","\n","        for i in range(0, total_images, batch_size):\n","            batch_files = image_files[i:i + batch_size]\n","            images = []\n","            for filename in batch_files:\n","                with z.open(filename) as file:\n","                    img = Image.open(BytesIO(file.read())).convert('RGB')\n","                    if transform:\n","                        img = transform(img)\n","                    images.append(img)\n","            yield torch.stack(images)\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-06-20T05:35:51.351189Z","iopub.status.busy":"2024-06-20T05:35:51.350901Z","iopub.status.idle":"2024-06-20T05:35:51.364476Z","shell.execute_reply":"2024-06-20T05:35:51.363684Z","shell.execute_reply.started":"2024-06-20T05:35:51.351145Z"},"trusted":true},"outputs":[],"source":["# import os\n","# from PIL import Image\n","# import torch\n","\n","# def image_batch_generator(root_folder, batch_size, transform=None):\n","#     all_image_files = []\n","\n","#     # Walk through all subdirectories and collect image files\n","#     for root, _, files in os.walk(root_folder):\n","#         for file in files:\n","#             if file.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n","#                 all_image_files.append(os.path.join(root, file))\n","\n","#     total_images = len(all_image_files)\n","    \n","#     for i in range(0, total_images, batch_size):\n","#         batch_files = all_image_files[i:i + batch_size]\n","#         images = []\n","#         for filename in batch_files:\n","#             img = Image.open(filename).convert('RGB')\n","#             if transform:\n","#                 img = transform(img)\n","#             images.append(torch.tensor(img))\n","#         yield torch.stack(images)\n"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-06-20T05:35:51.366744Z","iopub.status.busy":"2024-06-20T05:35:51.366476Z","iopub.status.idle":"2024-06-20T05:35:51.374405Z","shell.execute_reply":"2024-06-20T05:35:51.373557Z","shell.execute_reply.started":"2024-06-20T05:35:51.366721Z"},"trusted":true},"outputs":[],"source":["# zip_file_path = '/kaggle/input/art-portraits'\n","zip_file_path = 'archive.zip'\n","\n","transform = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n","])\n","\n","_dataloader = image_batch_generator(zip_file_path, batch_size, transform=transform)"]},{"cell_type":"markdown","metadata":{},"source":["## DESCRIMINATOR"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-06-20T05:35:51.375726Z","iopub.status.busy":"2024-06-20T05:35:51.375408Z","iopub.status.idle":"2024-06-20T05:35:51.388563Z","shell.execute_reply":"2024-06-20T05:35:51.387749Z","shell.execute_reply.started":"2024-06-20T05:35:51.375695Z"},"trusted":true},"outputs":[],"source":["class Critic(nn.Module):\n","    def __init__(self, input_channels):\n","        super(Critic, self).__init__()\n","\n","        self.channels = input_channels\n","\n","        self.conv1 = nn.Conv2d(self.channels, 32, kernel_size=3, stride=2, padding=1)\n","        self.activation1 = nn.LeakyReLU()\n","\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n","        self.activation2 = nn.LeakyReLU()\n","\n","        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n","        self.activation3 = nn.LeakyReLU()\n","\n","\n","        self.flatten = nn.Flatten(1,-1)\n","\n","        self.linear1 = nn.Linear(128 * 32 * 32, 2048)\n","        self.activation4 = nn.LeakyReLU()\n","        \n","        self.linear2 = nn.Linear(2048,1)\n","\n","    def forward(self,x):\n","        x = self.conv1(x)\n","        x = self.activation1(x)\n","        \n","        x = self.conv2(x)\n","        x = self.activation2(x)\n","\n","        x = self.conv3(x)\n","        x = self.activation3(x)\n","        \n","        x = self.flatten(x)\n","\n","        x = self.linear1(x)\n","        x = self.activation4(x)\n","        \n","        x = self.linear2(x)\n","\n","        return x\n","\n","    "]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-06-20T05:35:51.390042Z","iopub.status.busy":"2024-06-20T05:35:51.389650Z","iopub.status.idle":"2024-06-20T05:35:51.404034Z","shell.execute_reply":"2024-06-20T05:35:51.403203Z","shell.execute_reply.started":"2024-06-20T05:35:51.390004Z"},"trusted":true},"outputs":[],"source":["# decrimnator = Descriminator(input_channels=3)\n","# x = decrimnator(_dataloader.__next__())"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-06-20T05:35:51.405697Z","iopub.status.busy":"2024-06-20T05:35:51.405368Z","iopub.status.idle":"2024-06-20T05:35:51.414353Z","shell.execute_reply":"2024-06-20T05:35:51.413490Z","shell.execute_reply.started":"2024-06-20T05:35:51.405666Z"},"trusted":true},"outputs":[],"source":["# x.shape"]},{"cell_type":"markdown","metadata":{},"source":["# GENERATOR"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-06-20T05:35:55.281717Z","iopub.status.busy":"2024-06-20T05:35:55.280620Z","iopub.status.idle":"2024-06-20T05:35:55.298095Z","shell.execute_reply":"2024-06-20T05:35:55.297152Z","shell.execute_reply.started":"2024-06-20T05:35:55.281670Z"},"trusted":true},"outputs":[],"source":["import torch.nn as nn\n","\n","# output_shape = ( input - 1 ) * stride + output_padding - 2 * padding + kernel_size\n","\n","class Generator(nn.Module):\n","    def __init__(self, input_size, output_channels):\n","        super(Generator, self).__init__()\n","        self.input_size = input_size\n","        self.channels = output_channels\n","\n","        # self.conv1 = nn.ConvTranspose2d(input_size, 1024, 5, 2, 0, bias=False)\n","        # self.batchnorm1 = nn.BatchNorm2d(1024)\n","        # self.activation1 = nn.ReLU()\n","\n","        self.conv2 = nn.ConvTranspose2d(input_size, 512, 4, 2, 0, bias=False)\n","        self.batchnorm2 = nn.BatchNorm2d(512)\n","        self.activation2 = nn.ReLU(True)\n","\n","        self.conv3 = nn.ConvTranspose2d(512, 256, 3, 2, 1,output_padding = 1, bias=False)\n","        self.batchnorm3 = nn.BatchNorm2d(256)\n","        self.activation3 = nn.ReLU(True)\n","\n","        self.conv4 = nn.ConvTranspose2d(256, 256, 3, 2, 1,output_padding = 1, bias=False)\n","        self.batchnorm4 = nn.BatchNorm2d(256)\n","        self.activation4 = nn.ReLU(True)\n","\n","        self.conv5 = nn.ConvTranspose2d(256, 128, 3, 2, 1, output_padding = 1,bias=False)\n","        self.batchnorm5 = nn.BatchNorm2d(128)\n","        self.activation5 = nn.ReLU(True)\n","\n","        self.conv6 = nn.ConvTranspose2d(128, 64, 3, 2, 1, output_padding = 1,bias=False)\n","        self.batchnorm6 = nn.BatchNorm2d(64)\n","        self.activation6 = nn.ReLU(True)\n","\n","        self.conv7 = nn.ConvTranspose2d(64, 32, 3, 2, 1,output_padding = 1, bias=False)\n","        self.batchnorm7 = nn.BatchNorm2d(32)\n","        self.activation7 = nn.ReLU(True)\n","\n","        self.conv8 = nn.ConvTranspose2d(32, self.channels, 3, 2, 1, output_padding = 1,bias=False)\n","        self.batchnorm8 = nn.BatchNorm2d(self.channels)\n","        self.activation8 = nn.Tanh()\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","        x = x.view(batch_size, self.input_size, 1, 1)\n","        # x = self.conv1(x)\n","        # x = self.batchnorm1(x)\n","        # x = self.activation1(x)\n","\n","        x = self.conv2(x)\n","        x = self.batchnorm2(x)\n","        x = self.activation2(x)\n","\n","        x = self.conv3(x)\n","        x = self.batchnorm3(x)\n","        x = self.activation3(x)\n","\n","        x = self.conv4(x)\n","        x = self.batchnorm4(x)\n","        x = self.activation4(x)\n","\n","        x = self.conv5(x)\n","        x = self.batchnorm5(x)\n","        x = self.activation5(x)\n","\n","        x = self.conv6(x)\n","        x = self.batchnorm6(x)\n","        x = self.activation6(x)\n","\n","        x = self.conv7(x)\n","        x = self.batchnorm7(x)\n","        x = self.activation7(x)\n","\n","        x = self.conv8(x)\n","        x = self.batchnorm8(x)\n","        x = self.activation8(x)\n","       \n","        return x\n","    "]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-06-20T05:35:55.836664Z","iopub.status.busy":"2024-06-20T05:35:55.836355Z","iopub.status.idle":"2024-06-20T05:35:55.840534Z","shell.execute_reply":"2024-06-20T05:35:55.839602Z","shell.execute_reply.started":"2024-06-20T05:35:55.836640Z"},"trusted":true},"outputs":[],"source":["# generator_input = torch.randn(8, 100)  # Batch size of 64, input size of 100\n","# generator = Generator(input_size=100,output_channels=3)\n","# output = generator(generator_input)\n","# print(output.shape)"]},{"cell_type":"markdown","metadata":{},"source":["# GENERATOR USING  UPSAMPLING"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-06-20T05:35:57.154441Z","iopub.status.busy":"2024-06-20T05:35:57.153966Z","iopub.status.idle":"2024-06-20T05:35:57.170584Z","shell.execute_reply":"2024-06-20T05:35:57.169598Z","shell.execute_reply.started":"2024-06-20T05:35:57.154405Z"},"trusted":true},"outputs":[],"source":["import torch.nn as nn\n","\n","class UpGenerator(nn.Module):\n","    def __init__(self, input_size=100, output_channels=3):\n","        super(UpGenerator, self).__init__()\n","        self.input_size = input_size\n","        self.channels = output_channels\n","\n","        self.fc = nn.Linear(input_size, 512 * 8 * 8)\n","\n","        self.conv1 = nn.Conv2d(512, 256, 3, 1, 1, bias=False)\n","        self.batchnorm1 = nn.BatchNorm2d(256)\n","        self.activation1 = nn.ReLU()\n","\n","\n","        self.upsampling1 = nn.UpsamplingBilinear2d(scale_factor=2)\n","        self.conv2 = nn.Conv2d(256, 128, 3, 1, 1, bias=False)\n","        self.batchnorm2 = nn.BatchNorm2d(128)\n","        self.activation2 = nn.ReLU()\n","\n","        self.upsampling2 = nn.UpsamplingBilinear2d(scale_factor=2)\n","        self.conv3 = nn.Conv2d(128, 64, 3, 1, 1, bias=False)\n","        self.batchnorm3 = nn.BatchNorm2d(64)\n","        self.activation3 = nn.ReLU()\n","\n","        self.upsampling3 = nn.UpsamplingBilinear2d(scale_factor=2)\n","        self.conv4 = nn.Conv2d(64, 32, 3, 1, 1, bias=False)\n","        self.batchnorm4 = nn.BatchNorm2d(32)\n","        self.activation4 = nn.ReLU()\n","\n","        self.upsampling4 = nn.UpsamplingBilinear2d(scale_factor=2)\n","        self.conv5 = nn.Conv2d(32, 16, 3, 1, 1, bias=False)\n","        self.batchnorm5 = nn.BatchNorm2d(16)\n","        self.activation5 = nn.ReLU()\n","\n","        self.upsampling5 = nn.UpsamplingBilinear2d(scale_factor=2)\n","        self.conv6 = nn.Conv2d(16, self.channels, 3, 1, 1, bias=False)\n","        self.activation6 = nn.Tanh()\n","\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","        x = self.fc(x).view(batch_size, 512, 8, 8)\n","\n","        x = self.conv1(x)\n","        x = self.batchnorm1(x)\n","        x = self.activation1(x)\n","\n","        x = self.upsampling1(x)\n","        x = self.conv2(x)\n","        x = self.batchnorm2(x)\n","        x = self.activation2(x)\n","\n","        x = self.upsampling2(x)\n","        x = self.conv3(x)\n","        x = self.batchnorm3(x)\n","        x = self.activation3(x)\n","\n","        x = self.upsampling3(x)\n","        x = self.conv4(x)\n","        x = self.batchnorm4(x)\n","        x = self.activation4(x)\n","\n","        x = self.upsampling4(x)\n","        x = self.conv5(x)\n","        x = self.batchnorm5(x)\n","        x = self.activation5(x)\n","\n","        x = self.upsampling5(x)\n","        x = self.conv6(x)\n","        x = self.activation6(x)\n","\n","        return x\n","\n","# generator_input = torch.randn(8, 100)  # Batch size of 64, input size of 100\n","# generator = UpGenerator(input_size=100,output_channels=3)\n","# output = generator(generator_input)\n","# print(output.shape)"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-06-20T05:35:57.879198Z","iopub.status.busy":"2024-06-20T05:35:57.878557Z","iopub.status.idle":"2024-06-20T05:35:57.895458Z","shell.execute_reply":"2024-06-20T05:35:57.894403Z","shell.execute_reply.started":"2024-06-20T05:35:57.879151Z"},"trusted":true},"outputs":[],"source":["\n","class DCGAN(nn.Module):\n","    def __init__(self, critic,generator, latent_dim, device, gp_weight=10, lr_gen=0.0002, lr_disc=0.0002, betas=(0.5, 0.999),ratio = 3):\n","        super(DCGAN, self).__init__()\n","        self.generator = generator.to(device)\n","        self.critic = critic.to(device)\n","        self.latent_dim = latent_dim\n","        self.device = device\n","        self.gp_weight = gp_weight\n","\n","        self.optim_gen = optim.Adam(self.generator.parameters(), lr=lr_gen, betas=betas)\n","        self.optim_crit = optim.Adam(self.critic.parameters(), lr=lr_disc, betas=betas)\n","        \n","        self.criterion = nn.BCELoss()\n","        self.loss_fn = nn.BCELoss()\n","\n","        self.ratio = ratio\n","\n","        self.c_loss_metric = []\n","        self.c_wass_loss_metric = []\n","        self.c_gp_metric = []\n","        self.g_loss_metric = []\n","    \n","\n","    def gradient_penalty(self, batch_size, real_images, fake_images):\n","        alpha = torch.rand(batch_size, 1, 1, 1, device=self.device)\n","        interpolated = alpha * real_images + (1 - alpha) * fake_images\n","        interpolated.requires_grad_(True)\n","\n","        interpolated_predictions = self.critic(interpolated)\n","\n","        gradients = torch.autograd.grad(\n","            outputs=interpolated_predictions,\n","            inputs=interpolated,\n","            grad_outputs=torch.ones_like(interpolated_predictions, device=self.device),\n","            create_graph=True,\n","            retain_graph=True,\n","        )[0]\n","\n","        gradients = gradients.view(batch_size, -1)\n","        gradient_norm = gradients.norm(2, dim=1)\n","        gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n","        return gradient_penalty\n","    \n","\n","\n","    def train_step(self, real_images):\n","        batch_size = real_images.shape[0]\n","        real_images = real_images.to(self.device)\n","\n","\n","        for i in range(self.ratio):\n","            random_latent_vectors = torch.randn(batch_size, self.latent_dim, device=self.device)\n","            generated_images = self.generator(random_latent_vectors)\n","\n","            real_predictions = self.critic(real_images)\n","            fake_predictions = self.critic(generated_images.detach())  \n","            c_wass_loss = fake_predictions.mean() - real_predictions.mean() # diffrence in fake and real image score\n","\n","            c_gp = self.gradient_penalty(batch_size, real_images, generated_images)\n","            c_loss = c_wass_loss + c_gp * self.gp_weight\n","\n","            self.optim_crit.zero_grad()\n","            c_loss.backward()\n","            self.optim_crit.step()\n","\n","            # Update metrics\n","            self.c_loss_metric.append(c_loss.item())\n","            self.c_wass_loss_metric.append(c_wass_loss.item())\n","            self.c_gp_metric.append(c_gp.item())\n","\n","\n","        # Generator training\n","        random_latent_vectors = torch.randn(batch_size, self.latent_dim, device=self.device)\n","        fake_images = self.generator(random_latent_vectors)\n","        fake_predictions = self.critic(fake_images)\n","        g_loss = - fake_predictions.mean() # 0 - pred \n","\n","        self.optim_gen.zero_grad()\n","        g_loss.backward()\n","        self.optim_crit.step()\n","\n","        # Update metrics\n","        self.g_loss_metric.append(g_loss.item())\n","\n","        return {\n","            \"c_loss\": sum(self.c_loss_metric) / len(self.c_loss_metric),\n","            \"c_wass_loss\": sum(self.c_wass_loss_metric) / len(self.c_wass_loss_metric),\n","            \"c_gp\": sum(self.c_gp_metric) / len(self.c_gp_metric),\n","            \"g_loss\": sum(self.g_loss_metric) / len(self.g_loss_metric)\n","        }\n","\n","    \n","\n","    def generate_images(self, num_images=1, save_path=None,epoch=None):\n","        self.generator.eval()\n","        if epoch is None:\n","            epoch = \"_\"\t\n","            \n","        if os.path.exists(save_path) == False:\n","            os.makedirs(save_path)\n","\n","        with torch.no_grad():\n","            noise = torch.randn(num_images, self.latent_dim).to(self.device)\n","            generated_images = self.generator(noise)\n","            \n","            if save_path is not None:\n","                for i, image in enumerate(generated_images):\n","                    if os.path.exists(save_path) == False:\n","                        os.makedirs(save_path)\n","                    save_image(image, f\"{save_path}/EPOCH_{epoch}_image_{i}.png\")\n","            \n","            return generated_images"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["\n","descriminator = Critic(input_channels=_channels)\n","generator = Generator(input_size=latent_space,output_channels=_channels)\n","\n","DCGAN = DCGAN(descriminator, generator, latent_space, device,gp_weight=10, lr_gen=2e-3, lr_disc=2e-3, betas=(0.5, 0.999))"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["penatly tensor(0.9682, grad_fn=<MeanBackward0>)\n","penatly tensor(0.1044, grad_fn=<MeanBackward0>)\n","penatly tensor(5.4209, grad_fn=<MeanBackward0>)\n"]},{"data":{"text/plain":["{'c_loss': -134.77979882558188,\n"," 'c_wass_loss': -156.42507188611975,\n"," 'c_gp': 2.1645278483629227,\n"," 'g_loss': 1093.273193359375}"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["DCGAN.train_step(_dataloader.__next__())"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-06-20T05:36:00.230206Z","iopub.status.busy":"2024-06-20T05:36:00.229830Z","iopub.status.idle":"2024-06-20T05:36:05.381467Z","shell.execute_reply":"2024-06-20T05:36:05.380449Z","shell.execute_reply.started":"2024-06-20T05:36:00.230174Z"},"trusted":true},"outputs":[],"source":["def weights_init_glorot(m):\n","    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n","        nn.init.xavier_uniform_(m.weight)\n","        if m.bias is not None:\n","            nn.init.zeros_(m.bias)\n","\n","\n","descriminator = Critic(input_channels=_channels).apply(weights_init_glorot)\n","generator = Generator(input_size=latent_space,output_channels=_channels).apply(weights_init_glorot)\n","\n","DCGAN = DCGAN(descriminator, generator, latent_space, device, lr_gen=2e-3, lr_disc=2e-3, betas=(0.5, 0.999))"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-06-20T05:36:09.707304Z","iopub.status.busy":"2024-06-20T05:36:09.706956Z","iopub.status.idle":"2024-06-20T05:36:09.713819Z","shell.execute_reply":"2024-06-20T05:36:09.713001Z","shell.execute_reply.started":"2024-06-20T05:36:09.707277Z"},"trusted":true},"outputs":[],"source":["_dataloader = image_batch_generator(zip_file_path, 4, transform=transform)"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-06-20T05:36:12.312337Z","iopub.status.busy":"2024-06-20T05:36:12.311491Z","iopub.status.idle":"2024-06-20T05:36:13.286993Z","shell.execute_reply":"2024-06-20T05:36:13.285955Z","shell.execute_reply.started":"2024-06-20T05:36:12.312302Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(5.06100606918335, -0.13016650080680847, 0.5191172361373901)"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["DCGAN.train_step(_dataloader.__next__())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# import shutil\n","# import os\n","\n","# # Specify the path to the folder you want to delete\n","# folder_path = '/kaggle/working/generated_images'\n","\n","# # Check if the folder exists\n","# if os.path.exists(folder_path):\n","#     # Remove the folder and all its contents\n","#     shutil.rmtree(folder_path)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-06-20T05:36:28.566769Z","iopub.status.busy":"2024-06-20T05:36:28.566412Z","iopub.status.idle":"2024-06-20T05:56:16.185251Z","shell.execute_reply":"2024-06-20T05:56:16.183905Z","shell.execute_reply.started":"2024-06-20T05:36:28.566738Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_472/3432998535.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  images.append(torch.tensor(img))\n"]},{"name":"stdout","output_type":"stream","text":["epoch 0: {'d_loss': 1.3815033435821533, 'g_loss': 6.308757305145264}\n","epoch 1: {'d_loss': 1.3254845142364502, 'g_loss': 3.7382590770721436}\n","epoch 2: {'d_loss': 1.0476481914520264, 'g_loss': 4.312037944793701}\n","epoch 3: {'d_loss': 0.5357766151428223, 'g_loss': 4.355941295623779}\n","epoch :4 batch: 77\r"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m _dataloader \u001b[38;5;241m=\u001b[39m image_batch_generator(zip_file_path, \u001b[38;5;241m32\u001b[39m, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx,j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(_dataloader):\n\u001b[1;32m      5\u001b[0m     x \u001b[38;5;241m=\u001b[39m DCGAN\u001b[38;5;241m.\u001b[39mtrain_step(j)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch :\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m batch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,end \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[4], line 22\u001b[0m, in \u001b[0;36mimage_batch_generator\u001b[0;34m(root_folder, batch_size, transform)\u001b[0m\n\u001b[1;32m     20\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(filename)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m transform:\n\u001b[0;32m---> 22\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     images\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mtensor(img))\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(images)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    354\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:490\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    488\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    489\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m--> 490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:2193\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   2186\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2187\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2188\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2189\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2190\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2191\u001b[0m         )\n\u001b[0;32m-> 2193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["for i in range(num_epochs):\n","    x=0\n","    _dataloader = image_batch_generator(zip_file_path, 32, transform=transform)\n","    for idx,j in enumerate(_dataloader):\n","        x = DCGAN.train_step(j)\n","        print(f\"epoch :{i} batch: {idx}\",end =\"\\r\")\n","    print(f\"epoch {i}: {x}\")\n","    if i%15 == 0:\n","        torch.save(DCGAN.state_dict(), 'model_state_dict.pth')\n","    DCGAN.generate_images(num_images=1, save_path=\"generated_images\",epoch=i)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Save only the model's state dict\n","# torch.save(DCGAN.state_dict(), 'model_state_dict.pth')\n","# os.remove(\"/kaggle/working/model_state_dict.pth\")\n","# torch.save(DCGAN.state_dict(), 'model_state_dict.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["DCGAN.generate_images(num_images=1, save_path=\"generated_images\",epoch=4001)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sum(i.numel() for i in DCGAN.parameters())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1698586,"sourceId":7457578,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"DLENV","language":"python","name":"dlenv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"}},"nbformat":4,"nbformat_minor":4}
